{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression - my first tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    " * Explore the use of compression \n",
    " * Learn about chunking\n",
    " * Understand how dataset data is allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_H5PY=1  # set to 0 to use HDF Server instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_H5PY:\n",
    "    import h5py\n",
    "else:\n",
    "    import h5pyd as h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get folder/directory for HDF files we create  \n",
    "#\n",
    "def getMyFolder():\n",
    "    DIR_NAME = \"HDFLabTutorial/\"\n",
    "    if USE_H5PY:\n",
    "        myfolder = os.getenv(\"HOME\") + \"/\" + DIR_NAME\n",
    "        if not os.path.isdir(myfolder):\n",
    "            # create a directory on the local disk if needed\n",
    "            print(\"created folder:\", myfolder)\n",
    "            os.mkdir(myfolder)\n",
    "    else:\n",
    "        dir = h5py.Folder('/home/')  # get folder object for root\n",
    "        username = os.getenv(\"JUPYTERHUB_USER\")\n",
    "        myfolder = None\n",
    "        for name in dir:\n",
    "            # we should come across the given domain\n",
    "            if username.startswith(name):\n",
    "                # check any folders where the name matches at least part of the username\n",
    "                # e.g. folder: \"/home/bob/\" for username \"bob@acme.com\"\n",
    "                path = '/home/' + name + '/'\n",
    "                f = h5py.Folder(path)\n",
    "                if f.owner == username:\n",
    "                    myfolder = path\n",
    "                f.close()\n",
    "                if myfolder:\n",
    "                    break\n",
    "\n",
    "        dir.close()\n",
    "    \n",
    "        # create a workshop subfolder if not already present\n",
    "        myfolder += DIR_NAME\n",
    "        try:\n",
    "            h5py.Folder(myfolder)\n",
    "        except IOError as ioe:\n",
    "            if ioe.errno != 404:\n",
    "                return None  # unexpected error\n",
    "            # not present - create it now\n",
    "            h5py.Folder(myfolder, mode='x')\n",
    "            print(\"created folder:\", myfolder)\n",
    "       \n",
    "    return myfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/HDFLabTutorial/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get your home folder\n",
    "# will be a posix directory is H5PY is 1, or a server folder if 0\n",
    "home = getMyFolder()\n",
    "home  # this is the folder where you have permission to write to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file on the disk, or a domain on the server (based on USE_H5PY)\n",
    "filename = home + \"04a.h5\"\n",
    "f = h5py.File(filename, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72057594037927936"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.id.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66684702, 0.42902893, 0.40111429, ..., 0.51002711, 0.95600509,\n",
       "        0.51617687],\n",
       "       [0.4909785 , 0.03470718, 0.67728895, ..., 0.31901017, 0.39192206,\n",
       "        0.65180373],\n",
       "       [0.04044594, 0.34854649, 0.45788198, ..., 0.29439028, 0.7946077 ,\n",
       "        0.51152485],\n",
       "       ...,\n",
       "       [0.74341396, 0.91914145, 0.00918109, ..., 0.12316645, 0.83323736,\n",
       "        0.6971266 ],\n",
       "       [0.99814172, 0.74376858, 0.35049818, ..., 0.4942349 , 0.93584094,\n",
       "        0.64501134],\n",
       "       [0.88213715, 0.02006558, 0.45317104, ..., 0.19532329, 0.87759811,\n",
       "        0.91487925]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create some test data\n",
    "arr = np.random.rand(40, 80)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-compressed array\n",
    "dset1 = f.create_dataset('dset_nocompression', data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<f8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will return None for h5py, since chunks are not auto created for small datasets\n",
    "dset1.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset using the gzip filter\n",
    "dset2 = f.create_dataset('dset_gzip1', data=arr, compression='gzip', compression_opts=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another compressed dataset, but initialize it with data that is \n",
    "# easily compressed\n",
    "dset3 = f.create_dataset(\"dset_gzip2\", (40,80), dtype='f8', compression='gzip', compression_opts=9)\n",
    "dset3[...] = 42.0  # writes 42 to every element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()  # close file we can use h5ls on it (for h5py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dset_stats(filename, dsetname):\n",
    "    if USE_H5PY:\n",
    "        print(f\"Running h5ls to get dataset info\")\n",
    "        ! h5ls -v {filename}/{dsetname}\n",
    "    else:\n",
    "        # H5PYD has a num_chunks attribute to tell you how many chunks\n",
    "        # have been allocated\n",
    "        # num_chunks is determined asynchronously by the HDF Server. \n",
    "        # If num_chunks is 0, wait a few seconds and re-run the cell.\n",
    "        with h5py.File(filename) as f:\n",
    "            dset = f[dsetname]\n",
    "            logical_size = dset.dtype.itemsize\n",
    "            for dim in dset.shape:\n",
    "                logical_size *= dim\n",
    "            print(f\"logical size:   {logical_size}\")\n",
    "            if not dset.num_chunks:\n",
    "                print(\"No chunks found, if something has been written to this dataset, wait a few seconds and try this again\")\n",
    "            else:\n",
    "                chunk_size = dset.dtype.itemsize\n",
    "                for dim in dset.chunks:\n",
    "                    chunk_size *= dim\n",
    "                print(f\"Chunks: {dset.chunks} {chunk_size} bytes\")\n",
    "                # allocated size is also determined asynchronously, but \n",
    "                # is show be updated if num_chunks is\n",
    "                print(f\"allocated size: {dset.allocated_size}\")\n",
    "                ratio = logical_size/dset.allocated_size\n",
    "                ratio *= 100.0\n",
    "                print(f\"utilization: {ratio:.2f}%\")\n",
    "                print(f\"num_chunks: {dset.num_chunks}\")\n",
    "                if dset.compression:\n",
    "                    print(f\"Filter: {dset.compression} OPT: {dset.compression_opts}\")\n",
    "    print(\"-\"*40)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running h5ls to get dataset info\n",
      "Opened \"/home/jovyan/HDFLabTutorial/04a.h5\" with sec2 driver.\n",
      "dset_nocompression       Dataset {40/40, 80/80}\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "    Storage:   25600 logical bytes, 25600 allocated bytes, 100.00% utilization\n",
      "    Type:      native double\n",
      "----------------------------------------\n",
      "Running h5ls to get dataset info\n",
      "Opened \"/home/jovyan/HDFLabTutorial/04a.h5\" with sec2 driver.\n",
      "dset_gzip1               Dataset {40/40, 80/80}\n",
      "    Location:  1:1400\n",
      "    Links:     1\n",
      "    Chunks:    {20, 40} 6400 bytes\n",
      "    Storage:   25600 logical bytes, 24256 allocated bytes, 105.54% utilization\n",
      "    Filter-0:  deflate-1 OPT {9}\n",
      "    Type:      native double\n",
      "----------------------------------------\n",
      "Running h5ls to get dataset info\n",
      "Opened \"/home/jovyan/HDFLabTutorial/04a.h5\" with sec2 driver.\n",
      "dset_gzip2               Dataset {40/40, 80/80}\n",
      "    Location:  1:1672\n",
      "    Links:     1\n",
      "    Chunks:    {20, 40} 6400 bytes\n",
      "    Storage:   25600 logical bytes, 152 allocated bytes, 16842.11% utilization\n",
      "    Filter-0:  deflate-1 OPT {9}\n",
      "    Type:      native double\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_dset_stats(filename, \"dset_nocompression\")\n",
    "get_dset_stats(filename, \"dset_gzip1\")\n",
    "get_dset_stats(filename, \"dset_gzip2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF datasets are tiled into a a set of chunks.  This enables reading/writing data from storage to be done more efficiently.\n",
    "\n",
    "With the HDF5 native library, each chunk is stored in a contiguous section of the file.\n",
    "\n",
    "With HDF Server, each chunk is stored as a seperate S3 object. The chunk shape determines how many chunks will be used.  If not provided in create_dataset, the chunk layout will be determined automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datatset using the chunks option\n",
    "f = h5py.File(filename, \"a\")  # re-open in append mode\n",
    "dset4 = f.create_dataset(\"dset_chunks\", (40,80), dtype='i1', chunks=(4,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The library will faithfully use the chunk layout provided, but the \n",
    "# the server will take the chunk layout as a hint.\n",
    "dset4.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike with HDF5 library, the HDF Server may alter the inputed chunk layout so that each chunk is a reasonable size when stored in s3 (typically between 2-4MB).\n",
    "\n",
    "Try again with a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset5 = f.create_dataset(\"big_dset\", (4000,8000), dtype='i1', chunks=(4,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset5.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HDF Server, note that the \"shape\" of the chunk was preserved, but scaled up to hit the desired chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no actual chunks have been stored since we haven't written anything to the dataset.\n",
    "# write something to the dataset, this will initialize several chunks\n",
    "dset5[2000,:] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running h5ls to get dataset info\n",
      "Opened \"/home/jovyan/HDFLabTutorial/04a.h5\" with sec2 driver.\n",
      "dset_chunks              Dataset {40/40, 80/80}\n",
      "    Location:  1:57288\n",
      "    Links:     1\n",
      "    Chunks:    {4, 8} 32 bytes\n",
      "    Storage:   3200 logical bytes, 0 allocated bytes\n",
      "    Type:      native signed char\n",
      "----------------------------------------\n",
      "Running h5ls to get dataset info\n",
      "Opened \"/home/jovyan/HDFLabTutorial/04a.h5\" with sec2 driver.\n",
      "big_dset                 Dataset {4000/4000, 8000/8000}\n",
      "    Location:  1:57736\n",
      "    Links:     1\n",
      "    Chunks:    {4, 8} 32 bytes\n",
      "    Storage:   32000000 logical bytes, 32000 allocated bytes, 100000.00% utilization\n",
      "    Type:      native signed char\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "f.close()\n",
    "get_dset_stats(filename, \"dset_chunks\")\n",
    "get_dset_stats(filename, \"big_dset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
