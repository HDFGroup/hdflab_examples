{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    " * Use the h5pyd package to connect with the HDF Server\n",
    " * Explore characterstics of Datasets\n",
    " * Look at different ways of reading/writing to datasets\n",
    " * Examine how chunking works with HDF Server\n",
    " * Tricks for best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5pyd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USERNAME=\"myusername\"  # change this to your your username\n",
    "# create a file on the server\n",
    "filename = \"/home/\"+USERNAME+\"/workshop/03.h5\"\n",
    "f = h5pyd.File(filename, 'w')\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The 'w' mode removes and existing file (if any) and creates a new empty file.\n",
    "Other modes supported are:\n",
    " * 'r': Open as read only, file must exist\n",
    " * 'r+': Read/write, file must exist\n",
    " * 'x': Create file, fail if exist\n",
    " * 'a': Read/write if exists, otherwise create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The only object currently in the new file is the root group, we can get the id like this\n",
    "root = f['/']\n",
    "root.id.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.create_dataset(\"test1\", (3,4), dtype='i8')  # we've created a dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now something shows up if we list the contents of the file\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The dataset type is fixed at creation time\n",
    "dset = f['test1']\n",
    "dset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in this case the shape is fixed at create time, though we'll see later it is possible to\n",
    "# create extensible datasets\n",
    "dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can read all the elements of a dataset using the ellipsis operator\n",
    "out = dset[...]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can update portions of the dataset using numpy-like syntax\n",
    "dset[0,0:4] = [1,2,3,4]\n",
    "dset[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only portions of the dataset that actually get written are stored\n",
    "# create a really big dataset\n",
    "f.create_dataset(\"big_data\", (1024,1024,1024), dtype='f4')  # 4 GB dataset!\n",
    "dset = f['big_data']\n",
    "dset[512,512,512] = 3.12  # write one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read back a small region\n",
    "dset[510:514,512,512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Use hsls -H -v with this file.  At first you may see no storage allocatted for the file, but this will update in a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset storage is broken up into \"chunks\".  Each chunk is stored as a seperate S3 object\n",
    "# unlike with h5py, datasets are always chunked (even if it is just one chunk!).\n",
    "# Chunks are determined automtically if not provided in the dataset create call\n",
    "dset.chunks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify a chunk layout\n",
    "f.create_dataset(\"chunked_data\", (1024,1024,1024), dtype='f4',chunks=(1,1024,1024))\n",
    "dset = f[\"chunked_data\"]\n",
    "dset.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: The server will \"correct\" chunk layouts that result in chunks that are too small or too large.  Try creating datasets with very small and very large chunks.  What chunk layout do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete a dataset by using the del operator.  Unlike with the HDF5 library, this doesn't leave\n",
    "# \"holes\" in the file.  Only storage that is actually allocated is used\n",
    "del f['test1']\n",
    "list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you would like a default value other than 0, specify a \n",
    "#   fill value when creating the dataset\n",
    "f.create_dataset(\"fill_value\", (1024,1024,1024), dtype='i4', fillvalue=42)\n",
    "dset = f['fill_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset[1,2,3:6]  # get 3 elements from the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Run hsls -H -v with this file.  How many chunks in the dataset have been allocatted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open a data file\n",
    "f = h5pyd.File(\"/home/hdf/LOCA/tasmax_day_ACCESS1-0_historical_r1i1p1_19500101-19501231.LOCA_2016-04-02.16th.nc\", 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: what happens if you open this with the 'a' flag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a dataset\n",
    "dset = f['tasmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the dimensions\n",
    "dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read one slice of the dataset\n",
    "# For really large datasets it maybe impossible to read all the data into memory,\n",
    "# so often we will need to work with smaller pieces of the data\n",
    "\n",
    "# Time how long it takes to read 1 element\n",
    "side = 1\n",
    "%time arr = dset[123,0:side,0:side]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: How much longer does it take to read 100 elements (side=10)?  Or 10,000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sometimes it is useful to cull a (hopefully) representive sample of the data by\n",
    "# using a \"step\" value with the selection.  This grabs every \"step\" element.\n",
    "# We didn't give start and stop values in this case so start will be 0 and stop will be the \n",
    "# the size of the dimension\n",
    "\n",
    "arr = dset[123,::10,::10]\n",
    "arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: What are the dimensions of the returned array?\n",
    "\n",
    "Problem: Does the value of mean change much with different step values?  \n",
    "Would that be true for any dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way to select elements from a dataset is via \"point selection\".\n",
    "# When you provide a set of coordinates, you will get back a 1D list of selected \n",
    "# elements.\n",
    "coordinates = [(123,234,345),(246,46,69),(340,202,888)]\n",
    "dset[coordinates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you potentially need to expand a dataset, use the maxshape parameter \n",
    "# at creation time.\n",
    "f = h5pyd.File(filename, 'a')  # open original file in append mode\n",
    "dset = f.create_dataset('resizable', (2,3), maxshape=(20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.maxshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.resize((12, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an error will be returned if you try to go beyond the maxshape bounds...\n",
    "dset.resize((25,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what if you don't know the maximum size?  None values are interpreted as \"unlimited\" extent\n",
    "dset = f.create_dataset('unlimited', (2,3), maxshape=(2,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.maxshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.resize((2,200))\n",
    "dset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: What do expect the chunk shape to be for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
